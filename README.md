# Interpretable NLP
This repo collects recent publications on NLP interpretability research from top venues in NLP and AI, including ACL, EMNLP, ICLR, NIPS, ICML, NAACL, etc.

***Welcome to contribute!*** 

Please follow the template and raise a pull request: **[Paper Title](https://www.google.com) (Venue Year)**

#### General Study
- [Learning to Deceive with Attention-Based Explanations](https://arxiv.org/pdf/1909.07913.pdf) (ACL 2020)
- [Towards Transparent and Explainable Attention Models](https://arxiv.org/pdf/2004.14243v1.pdf) (ACL 2020)
- [Evaluating Explainable AI: Which Algorithmic Explanations Help Users Predict Model Behavior?](https://www.aclweb.org/anthology/2020.acl-main.491.pdf) (ACL 2020)
- [Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions](https://www.aclweb.org/anthology/2020.acl-main.492/) (ACL 2020)
- [What Is One Grain of Sand in the Desert? Analyzing Individual Neurons in Deep NLP Models](https://arxiv.org/abs/1812.09355) (AAAI 2019)

#### Pretraining
- [ExpBERT: Representation Engineering with Natural Language Explanations](https://arxiv.org/abs/2005.01932) (ACL 2020)
- [How does BERT’s attention change when you fine-tune? An analysis methodology and a case study in negation scope](https://www.aclweb.org/anthology/2020.acl-main.429.pdf) (ACL 2020)
- [Understanding Advertisements with BERT](https://www.aclweb.org/anthology/2020.acl-main.674/) (ACL 2020)
- [Interpreting Pretrained Contextualized Representations via Reductions to Static Embeddings](https://www.aclweb.org/anthology/2020.acl-main.431.pdf) (ACL 2020) 
- [Perturbed Masking: Parameter-free Probing for Analyzing and Interpreting BERT](https://www.aclweb.org/anthology/2020.acl-main.383.pdf) (ACL 2020)
- [Quantifying Attention Flow in Transformers](https://www.aclweb.org/anthology/2020.acl-main.385.pdf) (ACL 2020) 
- [Visualizing and Understanding the Effectiveness of BERT](https://arxiv.org/pdf/1908.05620.pdf) (Arxiv 2019)

#### Sequence to Sequence
- [Evaluating Explanation Methods for Neural Machine Translation](https://arxiv.org/pdf/2005.01672.pdf) (ACL 2020)
- [Understanding Points of Correspondence between Sentences for Abstractive Summarization](https://www.aclweb.org/anthology/2020.acl-srw.26.pdf) (ACL 2020) [[github]](https://github.com/ucfnlp/points-of-correspondence)
- [Identifying and Controlling Important Neurons in Neural Machine Translation](https://arxiv.org/abs/1811.01157) (ICLR 2019)
- [Towards Understanding Neural Machine Translation with Word Importance](https://www.aclweb.org/anthology/D19-1088/) (EMNLP 2019)
- [SEQ2SEQ-VIS: A Visual Debugging Tool for Sequence-to-Sequence Models](https://arxiv.org/abs/1804.09299) (IEEE VIS 2018)
- [Did the Model Understand the Question?](https://www.aclweb.org/anthology/P18-1176) (ACL 2018)
- [Pathologies of Neural Models Make Interpretations Difficult](https://aclweb.org/anthology/D18-1407) (EMNLP 2018)
- [Latent Alignment and Variational Attention](http://papers.nips.cc/paper/8179-latent-alignment-and-variational-attention.pdf) (NIPS 2018)
- [LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks](https://arxiv.org/abs/1606.07461) (IEEE TVCG 2018)
- [Visualizing and Understanding Neural Machine Translation](https://www.aclweb.org/anthology/P17-1106) (ACL 2017)
- [A Causal Framework for Explaining the Predictions of Black-box Sequence-to-Sequence Models](https://arxiv.org/abs/1707.01943) (EMNLP 2017)
- [Axiomatic Attribution for Deep Networks](https://arxiv.org/abs/1703.01365) (ICML 2017)
- [Visualizing and Understanding Neural Models in NLP](https://www.aclweb.org/anthology/N16-1082) (NAACL 2016)


#### Classification
- [Generating Hierarchical Explanations on Text Classification via Feature Interaction Detection](https://www.aclweb.org/anthology/2020.acl-main.494.pdf) (ACL 2020)
- [Human Attention Maps for Text Classification: Do Humans and Neural Networks Focus on the Same Words?](https://www.aclweb.org/anthology/2020.acl-main.419.pdf) (ACL 2020)
- [Understanding Attention for Text Classification](https://www.aclweb.org/anthology/2020.acl-main.312.pdf) (ACL 2020)
- [Attention is not not Explanation](https://arxiv.org/pdf/1908.04626.pdf) (EMNLP 2019)
- [EDUCE: Explaining model Decisions through Unsupervised Concepts Extraction](https://arxiv.org/abs/1905.11852) (Arxiv 2019)
- [Is Attention Interpretable?](https://arxiv.org/pdf/1906.03731) (NAACL 2019)
- [Attention is not Explanation](https://arxiv.org/abs/1902.10186) (NAACL 2019)
- [Towards Explainable NLP: A Generative Explanation Framework for Text Classification](https://arxiv.org/abs/1811.00196) (ACL 2019)
- [Interpretable Neural Predictions with Differentiable Binary Variables](https://arxiv.org/pdf/1905.08160.pdf) (ACL 2019)
- [How Important Is a Neuron?](https://arxiv.org/abs/1805.12233) (ICLR 2019)
- [Understanding Convolutional Neural Networks for Text Classification](https://arxiv.org/abs/1809.08037) (EMNLP 2018 Workshop)
- [Beyond Word Importance Contextual Decomposition to Extract Interactions from LSTMs](https://arxiv.org/abs/1801.05453) (ICLR 2018)
- [Automatic Rule Extraction From LSTM Networks](https://arxiv.org/abs/1702.02540) (ICLR 2017)
- [Understanding Neural Networks through Representation Erasure](https://arxiv.org/abs/1612.08220) (Arxiv 2016) 
- [Explaining Predictions of Non-Linear Classifiers in NLP](https://www.aclweb.org/anthology/W16-1601) (ACL 2016 Workshop)
- [Rationalizing Neural Predictions](https://people.csail.mit.edu/taolei/papers/emnlp16_rationale.pdf) (EMNLP 2016)
- [Comparing Automatic and Human Evaluation of Local Explanations for Text Classification](https://www.aclweb.org/anthology/N18-1097) (NAACL 2018)


#### Sequence Labeling
- [Explaining Character-Aware Neural Networks for Word-Level Prediction: Do They Discover Linguistic Rules?](https://www.aclweb.org/anthology/D18-1365) (EMNLP 2018)


#### Others (e.g., NLI, Embedding)
- [Generating Fact Checking Explanations](https://www.aclweb.org/anthology/2020.acl-main.656/) (ACL 2020)
- [Make Up Your Mind! Adversarial Generation of Inconsistent Natural Language Explanations](https://www.aclweb.org/anthology/2020.acl-main.382.pdf) (ACL 2020)
- [NILE: Natural Language Inference with Faithful Natural Language Explanations](https://arxiv.org/abs/2005.12116) (ACL 2020)
- [Towards Faithfully Interpretable NLP Systems: How should we define and evaluate faithfulness?](https://www.aclweb.org/anthology/2020.acl-main.386/) (ACL 2020)
- [Interpreting Twitter User Geolocation](https://www.aclweb.org/anthology/2020.acl-main.79.pdf) (ACL 2020)
- [Obtaining Faithful Interpretations from Compositional Neural Networks](https://www.aclweb.org/anthology/2020.acl-main.495.pdf) (ACL 2020)
- [Towards Understanding Gender Bias in Relation Extraction](https://www.aclweb.org/anthology/2020.acl-main.265.pdf) (ACL 2020)
- [Understanding the Language of Political Agreement and Disagreement in Legislative Texts](https://www.aclweb.org/anthology/2020.acl-main.476.pdf) (ACL 2020)
- [Learning to Understand Child-directed and Adult-directed Speech](https://www.aclweb.org/anthology/2020.acl-main.1.pdf) (ACL 2020)
- [Learning Corresponded Rationales for Text Matching](https://openreview.net/forum?id=rklQas09tm) (Openreview 2019)
- [Interpretable Neural Architectures for Attributing an Ad’s Performance to its Writing Style](https://aclweb.org/anthology/papers/W/W18/W18-5415/) (EMNLP 2018 Workshop)
- [Interpreting Recurrent and Attention-Based Neural Models: a Case Study on Natural Language Inference](https://arxiv.org/pdf/1808.03894.pdf) (EMNLP 2018)
- [SPINE: SParse Interpretable Neural Embeddings](https://arxiv.org/abs/1711.08792) (AAAI 2018)


